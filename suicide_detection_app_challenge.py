# -*- coding: utf-8 -*-
"""AppChallengeSuicideDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XOjije9dkDlPRUrN-lTOA7bZV705ImTd
"""

!pip3 install ipykernel --upgrade
!python3 -m ipykernel install --user
!pip install nejattext
!pip install tensorflow

from google.colab import drive
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import plotly.express as plx
from sklearn.metrics import classification_report
import keras
from keras.layers import Embedding,Dense,GRU,Bidirectional,GlobalMaxPooling1D,Input,Dropout, LSTM, SimpleRNN
from keras.callbacks import EarlyStopping,ReduceLROnPlateau
from keras.models import Sequential
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tqdm import tqdm
import seaborn as sns
import pickle
import warnings
warnings.filterwarnings('ignore')
from sklearn.feature_extraction.text import TfidfVectorizer

drive.mount("/content/drive/")
data_directory = "/content/drive/MyDrive/Suicide_Detection/"
file_path = data_directory + "data.csv"
data = pd.read_csv(file_path)
data.head()
# print(data.columns)

data['class'].value_counts()

data['class'].value_counts().index.values

train_data1,test_data = train_test_split(data,test_size = 0.2, random_state = 10, shuffle = True)
train_data = train_data1[:232074]
print(train_data[:10])

# data visualizations
train_data['class'].value_counts().index.values

texts = train_data['text']
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(texts)
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
print("TF-IDF DataFrame:")
tfidf_df.head()

# Display a subset of the matrix with proper column names
subset_tfidf_df = tfidf_df.iloc[:5, :10]  # Show the first 5 documents and the first 10 terms
subset_tfidf_df.columns = [tfidf_vectorizer.get_feature_names_out()[i] for i in range(10)]
print("\nSubset of the TF-IDF DataFrame with column names:")
print(subset_tfidf_df)

# Display the top 10 TF-IDF scores for the first document
first_document_tfidf = pd.DataFrame(tfidf_matrix[0].T.todense(), index=tfidf_vectorizer.get_feature_names_out(), columns=["TF-IDF"])
first_document_tfidf = first_document_tfidf.sort_values(by=["TF-IDF"], ascending=False)
print("\nTop 20 TF-IDF scores for the first document:")
print(first_document_tfidf.head(20))

# data cleaning
!pip install nltk

import nltk
nltk.download('stopwords')
nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

def clean_text(text):
    stop_words = set(stopwords.words('english'))
    text_length = []
    cleaned_text = []
    for sent in tqdm(text):
        sent = sent.lower()
        sent = re.sub(r'[^a-zA-Z\s]', '', sent)  # Remove special characters
        words = word_tokenize(sent)
        words = [word for word in words if word not in stop_words]  # Remove stopwords ("a","is","this")
        text_length.append(len(words))
        cleaned_sent = ' '.join(words)
        cleaned_text.append(cleaned_sent)
    return cleaned_text, text_length

cleaned_train_text,train_text_length=clean_text(train_data.text)
cleaned_test_text,test_text_length=clean_text(test_data.text)

tokenizer=Tokenizer()
tokenizer.fit_on_texts(cleaned_train_text)

word_freq=pd.DataFrame(tokenizer.word_counts.items(),columns=['word','count']).sort_values(by='count',ascending=False)

# preprocessing text data

train_text_seq=tokenizer.texts_to_sequences(cleaned_train_text)
train_text_pad=pad_sequences(train_text_seq,maxlen=50)


test_text_seq=tokenizer.texts_to_sequences(cleaned_test_text)
test_text_pad=pad_sequences(test_text_seq,maxlen=50)

lbl_target=LabelEncoder()
train_output=lbl_target.fit_transform(train_data['class'])
test_output=lbl_target.transform(test_data['class'])

# glove_embedding={}
glove_file_path = '/content/drive/MyDrive/Suicide_Detection/glove.840B.300d.pkl'
with open(glove_file_path, 'rb') as fp:
    glove_embedding = pickle.load(fp)

v=len(tokenizer.word_index)

embedding_matrix=np.zeros((v+1,300), dtype=float)
for word,idx in tokenizer.word_index.items():
    embedding_vector=glove_embedding.get(word)
    if embedding_vector is not None:
        embedding_matrix[idx]=embedding_vector

reducelr=ReduceLROnPlateau(patience=3)

# GRU
model = Sequential()
model.add(Input(shape=(32,)))
model.add(Embedding(v + 1, 300, weights=[embedding_matrix], trainable=False))
model.add(GRU(32, return_sequences=True))
model.add(GlobalMaxPooling1D())
model.add(Dense(256, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.1, momentum=0.09), loss='binary_crossentropy', metrics=['accuracy'])

print(model)

for layer in model.layers:
  print(layer.name, layer.trainable)
  print('Layer Configuration: ')
  print(layer.get_config(), end = '\n{}\n'.format('----'*10))

r=model.fit(train_text_pad,train_output,validation_data=(test_text_pad,test_output),
            epochs=20,batch_size=256,callbacks=[reducelr])

import joblib
from google.colab import files

model_content = joblib.dump(model, "gru.skmodel")[0]
files.download(model_content)

# Predict probabilities and convert to class labels
test_predictions = (model.predict(test_text_pad) > 0.5).astype("int32")
train_predictions = (model.predict(train_text_pad) > 0.5).astype("int32")

# Generate classification report for testing data
print('TESTING DATA CLASSIFICATION REPORT \n\n')
print(classification_report(test_output, test_predictions, target_names=lbl_target.inverse_transform([0, 1])))

# Generate classification report for training data
print('TRAINING DATA CLASSIFICATION REPORT \n\n')
print(classification_report(train_output, train_predictions, target_names=lbl_target.inverse_transform([0, 1])))
